app:
  name: ${APP_NAME:LLM Guard API}
  log_level: ${LOG_LEVEL:INFO}
  log_json: ${LOG_JSON:true}
  scan_fail_fast: ${SCAN_FAIL_FAST:false}
  scan_prompt_timeout: ${SCAN_PROMPT_TIMEOUT:120}  # Increased for model loading
  scan_output_timeout: ${SCAN_OUTPUT_TIMEOUT:120}  # Increased for model loading
  lazy_load: ${LAZY_LOAD:true}  # Models load on first request

rate_limit:
  enabled: ${RATE_LIMIT_ENABLED:false}
  limit: ${RATE_LIMIT_LIMIT:100/minute}

auth:
 type: http_bearer
 token: ${AUTH_TOKEN:}

tracing:
  exporter: ${TRACING_EXPORTER:console}
  endpoint: ${TRACING_OTEL_ENDPOINT:}  # Example: "<traces-endpoint>/v1/traces"

metrics:
  exporter: ${METRICS_TYPE:prometheus}
  endpoint: ${METRICS_ENDPOINT:}  # Example: "<metrics-endpoint>/v1/metrics"

# Scanners are applied in the order they are listed here.
input_scanners:
  # Primary scanner - Prompt Injection detection
  - type: PromptInjection
    params:
      threshold: 0.92
      match_type: truncate_head_tail
      model_max_length: 256
      # Optional: Use local model path if available (for Render disk or pre-downloaded models)
      # This can be overridden by PROMPT_INJECTION_MODEL_PATH environment variable
      model_path: ${PROMPT_INJECTION_MODEL_PATH:}


output_scanners: []  # Empty - we only care about input prompt injection detection
  # Commenting out all model-based scanners for now
  # - type: BanCode
  #   params:
  #     threshold: 0.97
  #     model_max_length: 256
  # - type: BanCompetitors
  #   params:
  #     competitors: ["facebook"]
  #     threshold: 0.75
  # - type: BanTopics
  #   params:
  #     topics: ["violence"]
  #     threshold: 0.8
  # - type: Bias
  #   params:
  #     threshold: 0.97
  #     model_max_length: 256
  # - type: FactualConsistency
  #   params:
  #     minimum_score: 0.5
  # - type: Gibberish
  #   params:
  #     threshold: 0.97
  # - type: Language
  #   params:
  #     valid_languages: [ "en" ]
  #     model_max_length: 256
  # - type: LanguageSame
  #   params:
  #     model_max_length: 256
  # - type: MaliciousURLs
  #   params:
  #     threshold: 0.75
  # - type: NoRefusal
  #   params:
  #     threshold: 0.9
  # - type: Relevance
  #   params:
  #     threshold: 0.2
  # - type: Sensitive
  #   params:
  #     redact: false
  #     threshold: 0.75
  # - type: Sentiment
  #   params:
  #     threshold: 0.0
  # - type: Toxicity
  #   params:
  #     threshold: 0.9
  #     model_max_length: 256
  # - type: EmotionDetection
  #   params:
  #     threshold: 0.5
  #     blocked_emotions: ["anger", "annoyance", "disappointment", "disapproval", "disgust", "embarrassment", "fear", "grief", "nervousness", "remorse", "sadness"]
  #     model_max_length: 256
  # - type: URLReachability
  #   params: {}
